# ğŸ“„ RAG-Powered Document Intelligence Assistant

A production-ready Retrieval-Augmented Generation (RAG) system that lets you upload documents and ask intelligent questions about their content. Features multiple chunking strategies, FAISS/ChromaDB vector stores, and support for OpenAI, Anthropic, and local Ollama models.

---

## âœ¨ Features

- **Multi-Format Document Ingestion** â€” PDF, DOCX, TXT, Markdown, CSV
- **4 Chunking Strategies** â€” Semantic, sentence-based, paragraph-based, and fixed-size
- **Dual Vector Store Support** â€” FAISS (fast, local) and ChromaDB (persistent, feature-rich)
- **Multiple LLM Providers** â€” OpenAI, Anthropic Claude, Ollama (fully local)
- **Interactive Streamlit UI** â€” Upload, chat, and explore your knowledge base
- **CLI Interface** â€” For scripting and terminal-based workflows
- **Source Attribution** â€” Every answer cites the exact document chunks used
- **Semantic Search Explorer** â€” Browse and inspect what the retriever finds

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  User Interface                  â”‚
â”‚          (Streamlit UI  /  CLI  /  API)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               RAG Pipeline                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Retrieverâ”‚â†’ â”‚ Context  â”‚â†’ â”‚  LLM Generate â”‚  â”‚
â”‚  â”‚ (Top-K)  â”‚  â”‚ Builder  â”‚  â”‚  (Answer)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Vector Store (FAISS/Chroma)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Embeddings  â”‚  â”‚  Document Chunks + Meta  â”‚  â”‚
â”‚  â”‚  (384/768d)  â”‚  â”‚                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Document Processor                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚PDF â”‚ â”‚DOCX â”‚ â”‚ TXT â”‚ â”‚ MD â”‚ â”‚ CSV â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â†“ Text Extraction â†“ Chunking            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd rag-assistant
pip install -r requirements.txt
```

### 2. Set Up Environment

```bash
cp .env.example .env
# Edit .env and add your API key (OpenAI, Anthropic, or use Ollama)
```

### 3. Run the Streamlit App

```bash
streamlit run app.py
```

### 4. Or Use the CLI

```bash
# Ingest documents
python cli.py ingest --path ./data

# Ask a question
python cli.py query "What are the key findings?"

# Interactive mode
python cli.py interactive

# View stats
python cli.py stats
```

---

## ğŸ“ Project Structure

```
rag-assistant/
â”œâ”€â”€ app.py                  # Streamlit web UI
â”œâ”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example            # Environment variable template
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_processor.py   # Document loading & chunking
â”‚   â”œâ”€â”€ vector_store.py         # FAISS & ChromaDB vector stores
â”‚   â””â”€â”€ rag_pipeline.py         # RAG orchestration & LLM integration
â”œâ”€â”€ data/                   # Place documents here for CLI ingestion
â”œâ”€â”€ uploads/                # Streamlit upload directory
â””â”€â”€ vectorstore/            # Persisted vector indices
```

---

## âš™ï¸ Configuration Guide

### Chunking Strategies

| Strategy      | Best For                              | Description                                      |
|---------------|---------------------------------------|--------------------------------------------------|
| `semantic`    | General documents                     | Splits by headers/sections, then by sentences     |
| `sentence`    | Narrative text, articles              | Respects sentence boundaries with overlap          |
| `paragraph`   | Well-structured docs with paragraphs  | Splits by paragraph breaks, merges short ones      |
| `fixed`       | Uniform processing                    | Fixed character windows with overlap               |

### Embedding Models

| Model                  | Dimensions | Speed  | Quality |
|------------------------|-----------|--------|---------|
| `all-MiniLM-L6-v2`    | 384       | âš¡ Fast | Good    |
| `all-mpnet-base-v2`   | 768       | ğŸ¢ Slower | Better |

### LLM Providers

| Provider   | Setup                                     | Cost       |
|------------|-------------------------------------------|------------|
| OpenAI     | Set `OPENAI_API_KEY` in `.env`            | Pay-per-use |
| Anthropic  | Set `ANTHROPIC_API_KEY` in `.env`         | Pay-per-use |
| Ollama     | Install Ollama + `ollama pull llama3.2`   | Free/local  |

---

## ğŸ”§ Query Modes

- **`answer`** â€” Standard Q&A with source citations
- **`summarize`** â€” Generate a summary of retrieved context
- **`compare`** â€” Compare information across different document sections
- **`extract`** â€” Extract structured data (returns JSON)

---

## ğŸ§ª Example Usage (Python API)

```python
from src.document_processor import DocumentProcessor
from src.vector_store import EmbeddingEngine, FAISSVectorStore
from src.rag_pipeline import RAGPipeline, OpenAIProvider

# 1. Process documents
processor = DocumentProcessor(chunk_size=512, chunking_strategy="semantic")
doc = processor.process("./data/report.pdf")

# 2. Build vector store
engine = EmbeddingEngine()
store = FAISSVectorStore(embedding_engine=engine)
store.add_chunks(doc.chunks)

# 3. Query
pipeline = RAGPipeline(
    vector_store=store,
    llm_provider=OpenAIProvider(model="gpt-4o-mini"),
)

response = pipeline.query("What are the key findings?")
print(response.answer)
print(response.format_sources())
```

---

## ğŸ“ License

MIT â€” use freely for personal and commercial projects.
